---
title: "Towards Lifelong Unseen Task Processing With a Lightweight Unlabeled Data Schema for AIoT"
collection: publications
category: Edge-Cloud-Synergy-AI
permalink: /publication/2024-05-06-Towards-Lifelong-Unseen-Task-Processing-With-a-Lightweight-Unlabeled-Data-Schema-for-AIoT
excerpt: 'A edge-cloud synergy AI framework addressing unseen tasks encountered by edge devices. [Project Link](https://github.com/kubeedge/ianvs/tree/main/examples/cityscapes/lifelong_learning_bench/unseen_task_processing-GANwithSelfTaughtLearning)'
date: 2024-05-06
venue: 'IEEE Internet of Things Journal'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://ieeexplore.ieee.org/abstract/document/10521590'
citation: '**Tianyu Tu**, Zhili He, Zhigao Zheng, Zimu Zheng, Jiawei Jiang, Yili Gong, Chuang Hu, Dazhao Cheng (2024). Towards Lifelong Unseen Task Processing With a Lightweight Unlabeled Data Schema for AIoT. IEEE Internet of Things Journal.'
---

With the rapid development of the Internet of Things (IoT), IoT devices find applications in various domains. The data generated by these devices is utilized for analysis and services, especially in the field of artificial intelligence (AI) applied to IoT, known as AI of Things (AIoT). The enhancement of edge device computing power in the IoT has led to the emergence of research areas like edge–cloud synergy AI theories and application services. In the context of lifelong learning and real-time processes in AIoT edge–cloud synergy services, addressing unseen tasks becomes crucial. Unseen tasks arise when inference requests from edge devices involve models not present in the cloud’s model repository. Addressing these challenges involves generating data to either augment small sample problems or alter the data distribution for heterogeneous sample issues. As the application of large language models (LLMs) for data generation gains traction, challenges emerge in the context of AIoT edge–cloud synergy services. First, fine-tuning LLMs with heterogeneous data exacerbates model bias issues. Second, the substantial data requirements for training LLMs pose a contradiction. Finally, the involvement of manual annotation in LLM-based data generation introduces complexity and cost. This article proposes a framework Seafarer to these challenges using generative adversarial networks and self-taught learning. Seafarer avoids model bias, reduces data requirements, and eliminates the need for manual annotation. The design demonstrates effectiveness theoretically and is validated on the Cityscapes data set, achieving an 80% reduction in training loss and improved validation loss stability.